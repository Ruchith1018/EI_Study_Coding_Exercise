The project focuses on developing a novel sampling strategy utilizing Reinforcement Learning (RL) to aid minimal supervision in machine learning tasks. In scenarios where labeled data is scarce and expensive to acquire, efficient data sampling becomes crucial. By framing the sampling challenge as a Markov Decision Process (MDP), we propose an RL-based framework that dynamically selects the most informative samples based on their expected contributions to model learning. Extensive experiments on benchmark datasets reveal that our approach significantly enhances model performance with limited labeled data compared to traditional sampling methods. The results underscore the potential of RL-driven strategies to optimize data utilization, enabling advancements in machine learning applications where supervision is constrained.
